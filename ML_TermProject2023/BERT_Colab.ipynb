{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"qYhOM9ErH4zF"},"source":["### gluonnlp 수정 ... 버전 업그레이드 시 vocab에 대한 init이 설정안되어있음"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lZfOlPDzH4oK"},"outputs":[],"source":["class BERTSentenceTransform:\n","    r\"\"\"BERT style data transformation.\n","\n","    Parameters\n","    ----------\n","    tokenizer : BERTTokenizer.\n","        Tokenizer for the sentences.\n","    max_seq_length : int.\n","        Maximum sequence length of the sentences.\n","    pad : bool, default True\n","        Whether to pad the sentences to maximum length.\n","    pair : bool, default True\n","        Whether to transform sentences or sentence pairs.\n","    \"\"\"\n","\n","    def __init__(self, tokenizer, max_seq_length, vocab, pad=True, pair=True):\n","        self._tokenizer = tokenizer\n","        self._max_seq_length = max_seq_length\n","        self._pad = pad\n","        self._pair = pair\n","        self._vocab = vocab\n","\n","    def __call__(self, line):\n","        \"\"\"Perform transformation for sequence pairs or single sequences.\n","\n","        The transformation is processed in the following steps:\n","        - tokenize the input sequences\n","        - insert [CLS], [SEP] as necessary\n","        - generate type ids to indicate whether a token belongs to the first\n","        sequence or the second sequence.\n","        - generate valid length\n","\n","        For sequence pairs, the input is a tuple of 2 strings:\n","        text_a, text_b.\n","\n","        Inputs:\n","            text_a: 'is this jacksonville ?'\n","            text_b: 'no it is not'\n","        Tokenization:\n","            text_a: 'is this jack ##son ##ville ?'\n","            text_b: 'no it is not .'\n","        Processed:\n","            tokens: '[CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]'\n","            type_ids: 0     0  0    0    0     0       0 0     1  1  1  1   1 1\n","            valid_length: 14\n","\n","        For single sequences, the input is a tuple of single string:\n","        text_a.\n","\n","        Inputs:\n","            text_a: 'the dog is hairy .'\n","        Tokenization:\n","            text_a: 'the dog is hairy .'\n","        Processed:\n","            text_a: '[CLS] the dog is hairy . [SEP]'\n","            type_ids: 0     0   0   0  0     0 0\n","            valid_length: 7\n","\n","        Parameters\n","        ----------\n","        line: tuple of str\n","            Input strings. For sequence pairs, the input is a tuple of 2 strings:\n","            (text_a, text_b). For single sequences, the input is a tuple of single\n","            string: (text_a,).\n","\n","        Returns\n","        -------\n","        np.array: input token ids in 'int32', shape (batch_size, seq_length)\n","        np.array: valid length in 'int32', shape (batch_size,)\n","        np.array: input token type ids in 'int32', shape (batch_size, seq_length)\n","\n","        \"\"\"\n","\n","        # convert to unicode\n","        text_a = line[0]\n","        if self._pair:\n","            assert len(line) == 2\n","            text_b = line[1]\n","\n","        # tokens_a = self._tokenizer(text_a)\n","        tokens_a = self._tokenizer.tokenize(text_a) #수정\n","        tokens_b = None\n","\n","        if self._pair:\n","            tokens_b = self._tokenizer(text_b)\n","\n","        if tokens_b:\n","            # Modifies `tokens_a` and `tokens_b` in place so that the total\n","            # length is less than the specified length.\n","            # Account for [CLS], [SEP], [SEP] with \"- 3\"\n","            self._truncate_seq_pair(tokens_a, tokens_b,\n","                                    self._max_seq_length - 3)\n","        else:\n","            # Account for [CLS] and [SEP] with \"- 2\"\n","            if len(tokens_a) > self._max_seq_length - 2:\n","                tokens_a = tokens_a[0:(self._max_seq_length - 2)]\n","\n","        # The embedding vectors for `type=0` and `type=1` were learned during\n","        # pre-training and are added to the wordpiece embedding vector\n","        # (and position vector). This is not *strictly* necessary since\n","        # the [SEP] token unambiguously separates the sequences, but it makes\n","        # it easier for the model to learn the concept of sequences.\n","\n","        # For classification tasks, the first vector (corresponding to [CLS]) is\n","        # used as as the \"sentence vector\". Note that this only makes sense because\n","        # the entire model is fine-tuned.\n","        vocab = self._vocab\n","        tokens = []\n","        tokens.append(vocab.cls_token)\n","        tokens.extend(tokens_a)\n","        tokens.append(vocab.sep_token)\n","        segment_ids = [0] * len(tokens)\n","\n","        if tokens_b:\n","            tokens.extend(tokens_b)\n","            tokens.append(vocab.sep_token)\n","            segment_ids.extend([1] * (len(tokens) - len(segment_ids)))\n","\n","        input_ids = self._tokenizer.convert_tokens_to_ids(tokens)\n","\n","        # The valid length of sentences. Only real  tokens are attended to.\n","        valid_length = len(input_ids)\n","\n","        if self._pad:\n","            # Zero-pad up to the sequence length.\n","            padding_length = self._max_seq_length - valid_length\n","            # use padding tokens for the rest\n","            input_ids.extend([vocab[vocab.padding_token]] * padding_length)\n","            segment_ids.extend([0] * padding_length)\n","\n","        return np.array(input_ids, dtype='int32'), np.array(valid_length, dtype='int32'),\\\n","            np.array(segment_ids, dtype='int32')"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"j9I9Zai3ICeK"},"source":["### 라이브러리 설치 및 import"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jLYEWmMdlD9m"},"outputs":[],"source":["!pip install xlsxwriter\n","!pip install konlpy\n","!pip install apyori\n","!pip install mxnet\n","!pip install gluonnlp==0.8.0\n","!pip install tqdm pandas\n","!pip install sentencepiece\n","!pip install transformers\n","!pip install torch\n","!pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":394},"executionInfo":{"elapsed":828,"status":"error","timestamp":1686022555001,"user":{"displayName":"HR Jang","userId":"13830645666148558230"},"user_tz":-540},"id":"kPR4PJFAjybD","outputId":"ae1062b1-f33b-4eca-bd6c-bc1ddf651e8b"},"outputs":[{"ename":"ModuleNotFoundError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-0561c3554911>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkobert_tokenizer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKoBERTTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBertModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'kobert_tokenizer'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}],"source":["import pandas as pd\n","import numpy as np\n","import os\n","import re\n","import progressbar\n","import networkx as nx\n","from konlpy.tag import Okt\n","from apyori import apriori\n","import matplotlib.pyplot as plt\n","\n","from kobert_tokenizer import KoBERTTokenizer\n","from transformers import BertModel\n","import torch\n","from torch import nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","import gluonnlp as nlp\n","from tqdm.notebook import tqdm\n","from transformers import AdamW\n","from transformers.optimization import get_cosine_schedule_with_warmup\n","from tqdm import tqdm, tqdm_notebook\n","\n","%matplotlib inline\n","path = '/content/drive/MyDrive/Project/Machine Learning/Keyword/'\n","\n","# Set GPU environment\n","device = torch.device(\"cuda:0\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"VYRiytBMIS0b"},"source":["### KoBERT"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"-pvyAi03J5dh"},"source":["#### 모델 로드 및 클래스 선언"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PBWW0a8bIRvf"},"outputs":[],"source":["# hugging fase Transformer로 KoBERT 모델 호출\n","tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n","bertmodel = BertModel.from_pretrained('skt/kobert-base-v1', return_dict=False)\n","vocab = nlp.vocab.BERTVocab.from_sentencepiece(tokenizer.vocab_file, padding_token='[PAD]')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"99T0qXJFIdpU"},"outputs":[],"source":["# Class 선언 및 Hyperparameter 설정\n","class BERTDataset(Dataset):\n","    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, vocab, max_len,\n","                 pad, pair):\n","        transform = nlp.data.BERTSentenceTransform(\n","            bert_tokenizer, max_seq_length=max_len, vocab=vocab, pad=pad, pair=pair)\n","\n","        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n","        self.labels = [np.int32(i[label_idx]) for i in dataset]\n","\n","    def __getitem__(self, i):\n","        return (self.sentences[i] + (self.labels[i], ))\n","\n","    def __len__(self):\n","        return (len(self.labels))\n","\n","class BERTClassifier(nn.Module):\n","    def __init__(self,\n","                 bert,\n","                 hidden_size = 768,\n","                 num_classes=5,\n","                 dr_rate=None,\n","                 params=None):\n","        super(BERTClassifier, self).__init__()\n","        self.bert = bert\n","        self.dr_rate = dr_rate\n","                 \n","        self.classifier = nn.Linear(hidden_size , num_classes)\n","        if dr_rate:\n","            self.dropout = nn.Dropout(p=dr_rate)\n","    \n","    def gen_attention_mask(self, token_ids, valid_length):\n","        attention_mask = torch.zeros_like(token_ids)\n","        for i, v in enumerate(valid_length):\n","            attention_mask[i][:v] = 1\n","        return attention_mask.float()\n","\n","    def forward(self, token_ids, valid_length, segment_ids):\n","        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n","        \n","        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n","        if self.dr_rate:\n","            out = self.dropout(pooler)\n","        return self.classifier(out)\n","\n","# Setting parameters\n","max_len = 64\n","batch_size = 64\n","warmup_ratio = 0.1\n","num_epochs = 5\n","max_grad_norm = 1\n","log_interval = 200\n","learning_rate =  5e-5"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"ukKQZPGhLLPF"},"source":["#### KoBERT 모델 학습 과정"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kytPL5WcoFGJ"},"outputs":[],"source":["# DataLoad 및 데이터 변환하여 설정\n","\n","dataset_train = nlp.data.TSVDataset('/content/drive/MyDrive/Project/LiveCommerse/OpenAI/DataFrame_train_5.tsv', field_indices=[0,1], num_discard_samples=1)\n","dataset_test = nlp.data.TSVDataset('/content/drive/MyDrive/Project/LiveCommerse/OpenAI/DataFrame_test_5.tsv', field_indices=[0,1], num_discard_samples=1)\n","\n","data_train = BERTDataset(dataset_train, 0, 1, tokenizer, vocab, max_len, True, False)\n","data_test = BERTDataset(dataset_test, 0, 1, tokenizer, vocab, max_len, True, False)\n","\n","train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=4)\n","test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=4)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VgDIpL5I6U02"},"outputs":[],"source":["# 모델 선언. GPU 사용\n","model = BERTClassifier(bertmodel, dr_rate=0.5).to(device)\n","\n","# Prepare optimizer and schedule\n","no_decay = ['bias', 'LayerNorm.weight']\n","optimizer_grouped_parameters = [\n","    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n","    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n","]\n","\n","optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n","loss_fn = nn.CrossEntropyLoss()\n","\n","t_total = len(train_dataloader) * num_epochs\n","warmup_step = int(t_total * warmup_ratio)\n","\n","scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)\n","\n","def calc_accuracy(X,Y):\n","    max_vals, max_indices = torch.max(X, 1)\n","    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n","    return train_acc"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PxRgWmOODBtq"},"outputs":[],"source":["# 모델 학습. 네이버쇼핑 채팅 데이터를 이용하여 감성 점수 예측 모델.\n","# 해당 모델은 gpt_3.5_turbo가 감성 점수를 부여한 데이터로 학습함.\n","\n","for e in range(num_epochs):\n","    train_acc = 0.0\n","    test_acc = 0.0\n","    model.train()\n","    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n","        optimizer.zero_grad()\n","        token_ids = token_ids.long().to(device)\n","        segment_ids = segment_ids.long().to(device)\n","        valid_length= valid_length\n","        label = label.long().to(device)\n","        out = model(token_ids, valid_length, segment_ids)\n","        loss = loss_fn(out, label)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n","        optimizer.step()\n","        scheduler.step()\n","        train_acc += calc_accuracy(out, label)\n","        if batch_id % log_interval == 0:\n","            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n","    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n","    model.eval()\n","    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):\n","        token_ids = token_ids.long().to(device)\n","        segment_ids = segment_ids.long().to(device)\n","        valid_length= valid_length\n","        label = label.long().to(device)\n","        out = model(token_ids, valid_length, segment_ids)\n","        test_acc += calc_accuracy(out, label)\n","    print(\"epoch {} validation acc {}\".format(e+1, test_acc / (batch_id+1)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BfpdycLvKz0T"},"outputs":[],"source":["# 모델 저장\n","PATH = '/content/drive/MyDrive/Project/Machine Learning/KoBERT/'\n","torch.save(model, PATH + 'KoBERT_GPT-turn.pt')\n","torch.save(model.state_dict(), PATH + 'model_state_dict.pt')\n","torch.save({\n","    'model': model.state_dict(),\n","    'optimizer': optimizer.state_dict()\n","}, PATH + 'all.tar')"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"k3_tqZLVLRJG"},"source":["#### 모델 불러와서 사용"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17832,"status":"ok","timestamp":1685809922150,"user":{"displayName":"HR Jang","userId":"13830645666148558230"},"user_tz":-540},"id":"835K-fx0xlVT","outputId":"445b624d-0b11-4a11-c339-0efb7f12c5e6"},"outputs":[{"data":{"text/plain":["<All keys matched successfully>"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["# 모델 로드. KoBERT Class 선언 먼저.\n","PATH = '/content/drive/MyDrive/Project/Machine Learning/KoBERT/'\n","model = torch.load(PATH + 'KoBERT_GPT-turn.pt')\n","model.load_state_dict(torch.load(PATH + 'model_state_dict.pt')) "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gYTB47sJxnu_"},"outputs":[],"source":["# KoBERT로 예측하는 함수 정의.\n","def predict(predict_sentence):\n","\n","    data = [predict_sentence, '0']\n","    dataset_another = [data]\n","\n","    another_test = BERTDataset(dataset_another, 0, 1, tokenizer, vocab, max_len, True, False)\n","    test_dataloader = torch.utils.data.DataLoader(another_test, batch_size=batch_size, num_workers=4)\n","    model.eval()\n","\n","    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):\n","        token_ids = token_ids.long().to(device) \n","        segment_ids = segment_ids.long().to(device) \n","        valid_length= valid_length \n","        out = model(token_ids, valid_length, segment_ids)\n","        prediction = out.cpu().detach().numpy().argmax()\n","    \n","    return prediction"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1671,"status":"ok","timestamp":1685809923814,"user":{"displayName":"HR Jang","userId":"13830645666148558230"},"user_tz":-540},"id":"d_TUq2RV_pFM","outputId":"8ae561e7-9ee0-49e3-cc14-8ac9d67b4c8f"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-80ae6136-909f-4aa1-acb7-df9171bf9ddb\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Rating</th>\n","      <th>Review_Text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>5</td>\n","      <td>배공빠르고 굿</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>택배가 엉망이네용 저희집 밑에층에 말도없이 놔두고가고</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>5</td>\n","      <td>아주좋아요 바지 정말 좋아서2개 더 구매했어요 이가격에 대박입니다. 바느질이 조금 ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2</td>\n","      <td>선물용으로 빨리 받아서 전달했어야 하는 상품이었는데 머그컵만 와서 당황했습니다. 전...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>민트색상 예뻐요. 옆 손잡이는 거는 용도로도 사용되네요 ㅎㅎ</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>199995</th>\n","      <td>2</td>\n","      <td>장마라그런가!!! 달지않아요</td>\n","    </tr>\n","    <tr>\n","      <th>199996</th>\n","      <td>5</td>\n","      <td>다이슨 케이스 구매했어요 다이슨 슈퍼소닉 드라이기 케이스 구매했어요가격 괜찮고 배송...</td>\n","    </tr>\n","    <tr>\n","      <th>199997</th>\n","      <td>5</td>\n","      <td>로드샾에서 사는것보다 세배 저렴하네요 ㅜㅜ 자주이용할께요</td>\n","    </tr>\n","    <tr>\n","      <th>199998</th>\n","      <td>5</td>\n","      <td>넘이쁘고 쎄련되보이네요~</td>\n","    </tr>\n","    <tr>\n","      <th>199999</th>\n","      <td>5</td>\n","      <td>아직 사용해보지도않았고 다른 제품을 써본적이없어서 잘 모르겠지만 ㅎㅎ 배송은 빨랐습니다</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>200000 rows × 2 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-80ae6136-909f-4aa1-acb7-df9171bf9ddb')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-80ae6136-909f-4aa1-acb7-df9171bf9ddb button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-80ae6136-909f-4aa1-acb7-df9171bf9ddb');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["        Rating                                        Review_Text\n","0            5                                            배공빠르고 굿\n","1            2                      택배가 엉망이네용 저희집 밑에층에 말도없이 놔두고가고\n","2            5  아주좋아요 바지 정말 좋아서2개 더 구매했어요 이가격에 대박입니다. 바느질이 조금 ...\n","3            2  선물용으로 빨리 받아서 전달했어야 하는 상품이었는데 머그컵만 와서 당황했습니다. 전...\n","4            5                  민트색상 예뻐요. 옆 손잡이는 거는 용도로도 사용되네요 ㅎㅎ\n","...        ...                                                ...\n","199995       2                                    장마라그런가!!! 달지않아요\n","199996       5  다이슨 케이스 구매했어요 다이슨 슈퍼소닉 드라이기 케이스 구매했어요가격 괜찮고 배송...\n","199997       5                    로드샾에서 사는것보다 세배 저렴하네요 ㅜㅜ 자주이용할께요\n","199998       5                                      넘이쁘고 쎄련되보이네요~\n","199999       5   아직 사용해보지도않았고 다른 제품을 써본적이없어서 잘 모르겠지만 ㅎㅎ 배송은 빨랐습니다\n","\n","[200000 rows x 2 columns]"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["# Naver_Shopping 데이터를 불러옴.\n","df = pd.read_table('/content/drive/MyDrive/Project/Machine Learning/corpus_bab2min/naver_shopping.txt',sep='\\t', names =['Rating', 'Review_Text'])\n","df"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vemiSmB4xtYq","outputId":"478e138f-f2d0-45dd-f185-d2f709f9a634"},"outputs":[{"name":"stderr","output_type":"stream","text":["\r  0% (0 of 200000) |                     | Elapsed Time: 0:00:00 ETA:  --:--:--/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","  0% (1 of 200000) |             | Elapsed Time: 0:00:04 ETA:  9 days, 20:18:46/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","  1% (2016 of 200000) |                  | Elapsed Time: 0:11:45 ETA:  17:55:29"]}],"source":["# 감성 점수 에측.\n","bar = progressbar.ProgressBar(maxval=len(df))\n","for i in range(len(df)):\n","  bar.update(i)\n","  review = df.loc[i, 'Review_Text']\n","  df.loc[i, 'KoBERT_Sentiment_Score'] = predict(review)\n","\n","bar.finish()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1a5taUAT_70z"},"outputs":[],"source":["# 저장\n","df.to_excel('/content/drive/MyDrive/Project/Machine Learning/Naver_Shopping_KoBERT_Senti.xlsx', index=False)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMkfLGDcqSyZkHltuf1UNwk","collapsed_sections":["qYhOM9ErH4zF","j9I9Zai3ICeK","-pvyAi03J5dh","ukKQZPGhLLPF","k3_tqZLVLRJG","6jrEO79VJjrB","YJK5_r1FhU9K","aOVTN7hbhRKW","vc3Ecdtp0Mqs","QSQhqslZ0OqB"],"machine_shape":"hm","mount_file_id":"1zFC9Cj3rPUhzdUc7_omsAJoWKX8yK12C","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
